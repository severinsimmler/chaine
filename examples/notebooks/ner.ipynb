{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6007d4cb-6917-47e8-9a35-faf74ceca728",
   "metadata": {},
   "source": [
    "# Named Entity Recognition with `chaine`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c536b91a-0d4f-498b-8f80-d19c35869efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "import chaine\n",
    "from chaine.typing import Dataset, Features, Sentence, Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa1d270-1363-4743-ae2b-aa38d74fa11b",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6726588b-57a0-4996-9d4c-6a0856826f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/home/severin/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35127df3120f4ff8b098b50f703ef62b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences for training: 14042\n",
      "Number of sentences for evaluation: 3454\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"conll2003\")\n",
    "\n",
    "print(f\"Number of sentences for training: {len(dataset['train']['tokens'])}\")\n",
    "print(f\"Number of sentences for evaluation: {len(dataset['test']['tokens'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e215fd6d-317b-4203-ba73-74cbb96f3feb",
   "metadata": {},
   "source": [
    "## Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b6b9eab-3928-4a8e-8cfa-03e8646b3156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_token(token_index: int, sentence: Sentence, pos_tags: Tags) -> Features:\n",
    "    \"\"\"Extract features from a token in a sentence.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    token_index : int\n",
    "        Index of the token to featurize in the sentence.\n",
    "    sentence : Sentence\n",
    "        Sequence of tokens.\n",
    "    pos_tags : Tags\n",
    "        Sequence of part-of-speech tags corresponding to the tokens in the sentence.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Features\n",
    "        Features representing the token.\n",
    "    \"\"\"\n",
    "    token = sentence[token_index]\n",
    "    pos_tag = pos_tags[token_index]\n",
    "    features = {\n",
    "        \"token.lower()\": token.lower(),\n",
    "        \"token[-3:]\": token[-3:],\n",
    "        \"token[-2:]\": token[-2:],\n",
    "        \"token.isupper()\": token.isupper(),\n",
    "        \"token.istitle()\": token.istitle(),\n",
    "        \"token.isdigit()\": token.isdigit(),\n",
    "        \"pos_tag\": pos_tag,\n",
    "    }\n",
    "    if token_index > 0:\n",
    "        previous_token = sentence[token_index - 1]\n",
    "        previous_pos_tag = pos_tags[token_index - 1]\n",
    "        features.update(\n",
    "            {\n",
    "                \"-1:token.lower()\": previous_token.lower(),\n",
    "                \"-1:token.istitle()\": previous_token.istitle(),\n",
    "                \"-1:token.isupper()\": previous_token.isupper(),\n",
    "                \"-1:pos_tag\": previous_pos_tag,\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        features[\"BOS\"] = True\n",
    "    if token_index < len(sentence) - 1:\n",
    "        next_token = sentence[token_index + 1]\n",
    "        next_pos_tag = pos_tags[token_index + 1]\n",
    "        features.update(\n",
    "            {\n",
    "                \"+1:token.lower()\": next_token.lower(),\n",
    "                \"+1:token.istitle()\": next_token.istitle(),\n",
    "                \"+1:token.isupper()\": next_token.isupper(),\n",
    "                \"+1:pos_tag\": next_pos_tag,\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        features[\"EOS\"] = True\n",
    "    return features\n",
    "\n",
    "\n",
    "def featurize_sentence(sentence: Sentence, pos_tags: Tags) -> list[Features]:\n",
    "    \"\"\"Extract features from tokens in a sentence.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence : Sentence\n",
    "        Sequence of tokens.\n",
    "    pos_tags : Tags\n",
    "        Sequence of part-of-speech tags corresponding to the tokens in the sentence.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[Features]\n",
    "        List of features representing tokens of a sentence.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        featurize_token(token_index, sentence, pos_tags) for token_index in range(len(sentence))\n",
    "    ]\n",
    "\n",
    "\n",
    "def featurize_dataset(dataset: Dataset) -> list[list[Features]]:\n",
    "    \"\"\"Extract features from sentences in a dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : Dataset\n",
    "        Dataset to featurize.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[list[Features]]\n",
    "        Featurized dataset.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        featurize_sentence(sentence, pos_tags)\n",
    "        for sentence, pos_tags in zip(dataset[\"tokens\"], dataset[\"pos_tags\"])\n",
    "    ]\n",
    "\n",
    "\n",
    "def preprocess_labels(dataset: Dataset) -> list[list[str]]:\n",
    "    \"\"\"Translate raw labels (i.e. integers) to the respective string labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : Dataset\n",
    "        Dataset to preprocess labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[list[Features]]\n",
    "        Preprocessed labels.\n",
    "    \"\"\"\n",
    "    labels = dataset.features[\"ner_tags\"].feature.names\n",
    "    return [[labels[index] for index in indices] for indices in dataset[\"ner_tags\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d520c53c-f8be-48a1-8484-1fafe81ad9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = featurize_dataset(dataset[\"train\"])\n",
    "train_labels = preprocess_labels(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab99001e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token.lower()': 'eu',\n",
       " 'token[-3:]': 'EU',\n",
       " 'token[-2:]': 'EU',\n",
       " 'token.isupper()': True,\n",
       " 'token.istitle()': False,\n",
       " 'token.isdigit()': False,\n",
       " 'pos_tag': 22,\n",
       " 'BOS': True,\n",
       " '+1:token.lower()': 'rejects',\n",
       " '+1:token.istitle()': False,\n",
       " '+1:token.isupper()': False,\n",
       " '+1:pos_tag': 42}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "105b64d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B-ORG'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d957af6e-ed83-4d55-90de-522c94151814",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3c91063-ca98-4685-8dcb-c30c637efc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = chaine.train(train_sentences, train_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8fdc83-8a8e-4eaa-b4f1-0395194db40a",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8c20257-3212-41bd-8018-b342b335acce",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = featurize_dataset(dataset[\"test\"])\n",
    "test_labels = preprocess_labels(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0309332-684d-460e-866a-92438ef5fb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.82      0.72      0.77      1668\n",
      "        MISC       0.66      0.66      0.66       702\n",
      "         ORG       0.70      0.59      0.64      1661\n",
      "         PER       0.82      0.77      0.79      1617\n",
      "\n",
      "   micro avg       0.76      0.69      0.72      5648\n",
      "   macro avg       0.75      0.69      0.72      5648\n",
      "weighted avg       0.76      0.69      0.72      5648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_sentences)\n",
    "\n",
    "print(classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb319d4-9ce6-4a74-9792-17b8ae131c1e",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1569a1f1-212c-4fc2-b8d3-b601b5aea88c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-05-25 09:23:22,872] [INFO] Starting with arow (1/5)\n",
      "[2022-05-25 09:23:22,873] [INFO] Baseline for arow\n",
      "[2022-05-25 09:24:20,396] [INFO] Trial 1/10 for arow\n",
      "[2022-05-25 09:25:29,783] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 09:25:29,784] [INFO] Best optimized model: 0.8909010732379357\n",
      "[2022-05-25 09:25:29,784] [INFO] Trial 2/10 for arow\n",
      "[2022-05-25 09:26:37,112] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 09:26:37,113] [INFO] Best optimized model: 0.8978907390633878\n",
      "[2022-05-25 09:26:37,114] [INFO] Trial 3/10 for arow\n",
      "[2022-05-25 09:27:56,355] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 09:27:56,355] [INFO] Best optimized model: 0.8978907390633878\n",
      "[2022-05-25 09:27:56,356] [INFO] Trial 4/10 for arow\n",
      "[2022-05-25 09:29:04,767] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 09:29:04,768] [INFO] Best optimized model: 0.8978907390633878\n",
      "[2022-05-25 09:29:04,769] [INFO] Trial 5/10 for arow\n",
      "[2022-05-25 09:30:12,147] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 09:30:12,148] [INFO] Best optimized model: 0.8978907390633878\n",
      "[2022-05-25 09:30:12,149] [INFO] Trial 6/10 for arow\n",
      "[2022-05-25 09:31:50,378] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 09:31:50,380] [INFO] Best optimized model: 0.8978907390633878\n",
      "[2022-05-25 09:31:50,381] [INFO] Trial 7/10 for arow\n",
      "[2022-05-25 09:33:17,917] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 09:33:17,917] [INFO] Best optimized model: 0.8978907390633878\n",
      "[2022-05-25 09:33:17,918] [INFO] Trial 8/10 for arow\n",
      "[2022-05-25 09:34:33,506] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 09:34:33,506] [INFO] Best optimized model: 0.8978907390633878\n",
      "[2022-05-25 09:34:33,507] [INFO] Trial 9/10 for arow\n",
      "[2022-05-25 09:36:11,384] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 09:36:11,385] [INFO] Best optimized model: 0.8978907390633878\n",
      "[2022-05-25 09:36:11,386] [INFO] Trial 10/10 for arow\n",
      "[2022-05-25 09:37:42,740] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 09:37:42,740] [INFO] Best optimized model: 0.8978907390633878\n",
      "[2022-05-25 09:37:42,741] [INFO] Starting with ap (2/5)\n",
      "[2022-05-25 09:37:42,741] [INFO] Baseline for ap\n",
      "[2022-05-25 09:39:24,452] [INFO] Trial 1/10 for ap\n",
      "[2022-05-25 09:40:53,091] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 09:40:53,092] [INFO] Best optimized model: 0.8978907390633878\n",
      "[2022-05-25 09:40:53,093] [INFO] Trial 2/10 for ap\n",
      "[2022-05-25 09:42:21,499] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 09:42:21,500] [INFO] Best optimized model: 0.8978907390633878\n",
      "[2022-05-25 09:42:21,500] [INFO] Trial 3/10 for ap\n",
      "[2022-05-25 09:44:11,890] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 09:44:11,890] [INFO] Best optimized model: 0.8978907390633878\n",
      "[2022-05-25 09:44:11,891] [INFO] Trial 4/10 for ap\n",
      "[2022-05-25 09:46:02,012] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 09:46:02,014] [INFO] Best optimized model: 0.8978907390633878\n",
      "[2022-05-25 09:46:02,014] [INFO] Trial 5/10 for ap\n",
      "[2022-05-25 09:47:53,548] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 09:47:53,548] [INFO] Best optimized model: 0.8978907390633878\n",
      "[2022-05-25 09:47:53,549] [INFO] Trial 6/10 for ap\n",
      "[2022-05-25 09:49:20,355] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 09:49:20,356] [INFO] Best optimized model: 0.8978907390633878\n",
      "[2022-05-25 09:49:20,357] [INFO] Trial 7/10 for ap\n",
      "[2022-05-25 09:51:24,878] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 09:51:24,878] [INFO] Best optimized model: 0.8978907390633878\n",
      "[2022-05-25 09:51:24,879] [INFO] Trial 8/10 for ap\n",
      "[2022-05-25 09:52:58,025] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 09:52:58,026] [INFO] Best optimized model: 0.8978907390633878\n",
      "[2022-05-25 09:52:58,027] [INFO] Trial 9/10 for ap\n",
      "[2022-05-25 09:54:38,021] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 09:54:38,021] [INFO] Best optimized model: 0.8978907390633878\n",
      "[2022-05-25 09:54:38,022] [INFO] Trial 10/10 for ap\n",
      "[2022-05-25 09:56:20,251] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 09:56:20,252] [INFO] Best optimized model: 0.8978907390633878\n",
      "[2022-05-25 09:56:20,252] [INFO] Starting with lbfgs (3/5)\n",
      "[2022-05-25 09:56:20,253] [INFO] Baseline for lbfgs\n",
      "[2022-05-25 09:58:15,855] [INFO] Trial 1/10 for lbfgs\n",
      "[2022-05-25 10:00:05,039] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 10:00:05,040] [INFO] Best optimized model: 0.8978907390633878\n",
      "[2022-05-25 10:00:05,041] [INFO] Trial 2/10 for lbfgs\n",
      "[2022-05-25 10:02:02,479] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 10:02:02,481] [INFO] Best optimized model: 0.8978907390633878\n",
      "[2022-05-25 10:02:02,481] [INFO] Trial 3/10 for lbfgs\n",
      "[2022-05-25 10:03:47,661] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 10:03:47,662] [INFO] Best optimized model: 0.9058707702090455\n",
      "[2022-05-25 10:03:47,663] [INFO] Trial 4/10 for lbfgs\n",
      "[2022-05-25 10:05:43,615] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 10:05:43,617] [INFO] Best optimized model: 0.9058707702090455\n",
      "[2022-05-25 10:05:43,618] [INFO] Trial 5/10 for lbfgs\n",
      "[2022-05-25 10:08:31,421] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 10:08:31,421] [INFO] Best optimized model: 0.9225488624502322\n",
      "[2022-05-25 10:08:31,422] [INFO] Trial 6/10 for lbfgs\n",
      "[2022-05-25 10:10:20,815] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 10:10:20,816] [INFO] Best optimized model: 0.9225488624502322\n",
      "[2022-05-25 10:10:20,817] [INFO] Trial 7/10 for lbfgs\n",
      "[2022-05-25 10:12:38,246] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 10:12:38,247] [INFO] Best optimized model: 0.9225488624502322\n",
      "[2022-05-25 10:12:38,248] [INFO] Trial 8/10 for lbfgs\n",
      "[2022-05-25 10:14:49,489] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 10:14:49,489] [INFO] Best optimized model: 0.9225488624502322\n",
      "[2022-05-25 10:14:49,490] [INFO] Trial 9/10 for lbfgs\n",
      "[2022-05-25 10:16:57,773] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 10:16:57,774] [INFO] Best optimized model: 0.9225488624502322\n",
      "[2022-05-25 10:16:57,776] [INFO] Trial 10/10 for lbfgs\n",
      "[2022-05-25 10:18:55,195] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 10:18:55,196] [INFO] Best optimized model: 0.9225488624502322\n",
      "[2022-05-25 10:18:55,197] [INFO] Starting with l2sgd (4/5)\n",
      "[2022-05-25 10:18:55,199] [INFO] Baseline for l2sgd\n",
      "[2022-05-25 10:21:23,637] [INFO] Trial 1/10 for l2sgd\n",
      "[2022-05-25 10:23:20,241] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 10:23:20,241] [INFO] Best optimized model: 0.9225488624502322\n",
      "[2022-05-25 10:23:20,242] [INFO] Trial 2/10 for l2sgd\n",
      "[2022-05-25 10:26:09,872] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 10:26:09,872] [INFO] Best optimized model: 0.9225488624502322\n",
      "[2022-05-25 10:26:09,873] [INFO] Trial 3/10 for l2sgd\n",
      "[2022-05-25 10:28:42,254] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 10:28:42,255] [INFO] Best optimized model: 0.9225488624502322\n",
      "[2022-05-25 10:28:42,256] [INFO] Trial 4/10 for l2sgd\n",
      "[2022-05-25 10:31:16,728] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 10:31:16,729] [INFO] Best optimized model: 0.9225488624502322\n",
      "[2022-05-25 10:31:16,730] [INFO] Trial 5/10 for l2sgd\n",
      "[2022-05-25 10:34:19,426] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 10:34:19,427] [INFO] Best optimized model: 0.9225488624502322\n",
      "[2022-05-25 10:34:19,428] [INFO] Trial 6/10 for l2sgd\n",
      "[2022-05-25 10:37:00,452] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 10:37:00,453] [INFO] Best optimized model: 0.9225488624502322\n",
      "[2022-05-25 10:37:00,453] [INFO] Trial 7/10 for l2sgd\n",
      "[2022-05-25 10:39:40,710] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 10:39:40,711] [INFO] Best optimized model: 0.9225488624502322\n",
      "[2022-05-25 10:39:40,711] [INFO] Trial 8/10 for l2sgd\n",
      "[2022-05-25 10:42:25,353] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 10:42:25,353] [INFO] Best optimized model: 0.9225488624502322\n",
      "[2022-05-25 10:42:25,354] [INFO] Trial 9/10 for l2sgd\n",
      "[2022-05-25 10:45:11,160] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 10:45:11,161] [INFO] Best optimized model: 0.9225488624502322\n",
      "[2022-05-25 10:45:11,162] [INFO] Trial 10/10 for l2sgd\n",
      "[2022-05-25 10:47:33,224] [INFO] Best baseline model: 0.8538541405229366\n",
      "[2022-05-25 10:47:33,225] [INFO] Best optimized model: 0.9225488624502322\n",
      "[2022-05-25 10:47:33,225] [INFO] Starting with pa (5/5)\n",
      "[2022-05-25 10:47:33,226] [INFO] Baseline for pa\n",
      "[2022-05-25 10:49:37,885] [INFO] Trial 1/10 for pa\n",
      "[2022-05-25 10:52:10,237] [INFO] Best baseline model: 0.8673452502903771\n",
      "[2022-05-25 10:52:10,237] [INFO] Best optimized model: 0.9225488624502322\n",
      "[2022-05-25 10:52:10,238] [INFO] Trial 2/10 for pa\n",
      "[2022-05-25 10:54:10,697] [INFO] Best baseline model: 0.8673452502903771\n",
      "[2022-05-25 10:54:10,698] [INFO] Best optimized model: 0.9225488624502322\n",
      "[2022-05-25 10:54:10,699] [INFO] Trial 3/10 for pa\n",
      "[2022-05-25 10:56:31,984] [INFO] Best baseline model: 0.8673452502903771\n",
      "[2022-05-25 10:56:31,985] [INFO] Best optimized model: 0.9225488624502322\n",
      "[2022-05-25 10:56:31,986] [INFO] Trial 4/10 for pa\n",
      "[2022-05-25 10:59:04,346] [INFO] Best baseline model: 0.8673452502903771\n",
      "[2022-05-25 10:59:04,348] [INFO] Best optimized model: 0.9225488624502322\n",
      "[2022-05-25 10:59:04,349] [INFO] Trial 5/10 for pa\n",
      "[2022-05-25 11:01:31,107] [INFO] Best baseline model: 0.8673452502903771\n",
      "[2022-05-25 11:01:31,108] [INFO] Best optimized model: 0.9225488624502322\n",
      "[2022-05-25 11:01:31,109] [INFO] Trial 6/10 for pa\n",
      "[2022-05-25 11:03:38,579] [INFO] Best baseline model: 0.8673452502903771\n",
      "[2022-05-25 11:03:38,579] [INFO] Best optimized model: 0.9225488624502322\n",
      "[2022-05-25 11:03:38,580] [INFO] Trial 7/10 for pa\n",
      "[2022-05-25 11:05:16,477] [INFO] Best baseline model: 0.8673452502903771\n",
      "[2022-05-25 11:05:16,478] [INFO] Best optimized model: 0.9225488624502322\n",
      "[2022-05-25 11:05:16,479] [INFO] Trial 8/10 for pa\n",
      "[2022-05-25 11:06:55,920] [INFO] Best baseline model: 0.8673452502903771\n",
      "[2022-05-25 11:06:55,921] [INFO] Best optimized model: 0.9225488624502322\n",
      "[2022-05-25 11:06:55,922] [INFO] Trial 9/10 for pa\n",
      "[2022-05-25 11:08:47,068] [INFO] Best baseline model: 0.8673452502903771\n",
      "[2022-05-25 11:08:47,069] [INFO] Best optimized model: 0.9225488624502322\n",
      "[2022-05-25 11:08:47,069] [INFO] Trial 10/10 for pa\n",
      "[2022-05-25 11:10:34,852] [INFO] Best baseline model: 0.8673452502903771\n",
      "[2022-05-25 11:10:34,853] [INFO] Best optimized model: 0.9225488624502322\n",
      "[2022-05-25 11:10:34,854] [INFO] Finished hyperparameter optimization\n",
      "[2022-05-25 11:10:34,854] [INFO] Trained 55 models with different hyperparamters\n",
      "[2022-05-25 11:10:34,858] [INFO] Loading data set\n",
      "[2022-05-25 11:10:38,048] [INFO] Start training\n",
      "[2022-05-25 11:10:38,049] [INFO] Processing training data\n",
      "[2022-05-25 11:10:38,437] [INFO] Processed 10% of the training data\n",
      "[2022-05-25 11:10:38,900] [INFO] Processed 20% of the training data\n",
      "[2022-05-25 11:10:39,269] [INFO] Processed 30% of the training data\n",
      "[2022-05-25 11:10:39,888] [INFO] Processed 40% of the training data\n",
      "[2022-05-25 11:10:40,625] [INFO] Processed 50% of the training data\n",
      "[2022-05-25 11:10:41,142] [INFO] Processed 60% of the training data\n",
      "[2022-05-25 11:10:41,607] [INFO] Processed 70% of the training data\n",
      "[2022-05-25 11:10:42,068] [INFO] Processed 80% of the training data\n",
      "[2022-05-25 11:10:42,818] [INFO] Processed 90% of the training data\n",
      "[2022-05-25 11:10:43,789] [INFO] Processed 100% of the training data\n",
      "[2022-05-25 11:10:43,917] [INFO] Start training with L-BFGS\n",
      "[2022-05-25 11:10:44,773] [INFO] Iteration 1, training loss: 283388.543569\n",
      "[2022-05-25 11:10:44,988] [INFO] Iteration 2, training loss: 238571.509453\n",
      "[2022-05-25 11:10:45,701] [INFO] Iteration 3, training loss: 185707.832694\n",
      "[2022-05-25 11:10:45,966] [INFO] Iteration 4, training loss: 182427.956479\n",
      "[2022-05-25 11:10:46,242] [INFO] Iteration 5, training loss: 170732.563357\n",
      "[2022-05-25 11:10:46,504] [INFO] Iteration 6, training loss: 168563.550995\n",
      "[2022-05-25 11:10:46,762] [INFO] Iteration 7, training loss: 167759.381547\n",
      "[2022-05-25 11:10:47,249] [INFO] Iteration 8, training loss: 166847.089946\n",
      "[2022-05-25 11:10:47,767] [INFO] Iteration 9, training loss: 165213.160279\n",
      "[2022-05-25 11:10:48,289] [INFO] Iteration 10, training loss: 164087.203206\n",
      "[2022-05-25 11:10:48,597] [INFO] Iteration 11, training loss: 162971.724909\n",
      "[2022-05-25 11:10:48,873] [INFO] Iteration 12, training loss: 162056.506561\n",
      "[2022-05-25 11:10:49,163] [INFO] Iteration 13, training loss: 161139.053315\n",
      "[2022-05-25 11:10:49,469] [INFO] Iteration 14, training loss: 159714.792042\n",
      "[2022-05-25 11:10:49,761] [INFO] Iteration 15, training loss: 155288.977768\n",
      "[2022-05-25 11:10:50,276] [INFO] Iteration 16, training loss: 138416.116654\n",
      "[2022-05-25 11:10:50,831] [INFO] Iteration 17, training loss: 116236.961930\n",
      "[2022-05-25 11:10:51,326] [INFO] Iteration 18, training loss: 110493.601250\n",
      "[2022-05-25 11:10:51,578] [INFO] Iteration 19, training loss: 103910.508091\n",
      "[2022-05-25 11:10:51,881] [INFO] Iteration 20, training loss: 99699.490169\n",
      "[2022-05-25 11:10:52,178] [INFO] Iteration 21, training loss: 93898.330349\n",
      "[2022-05-25 11:10:52,416] [INFO] Iteration 22, training loss: 88866.008350\n",
      "[2022-05-25 11:10:52,735] [INFO] Iteration 23, training loss: 85558.046157\n",
      "[2022-05-25 11:10:53,257] [INFO] Iteration 24, training loss: 80590.830862\n",
      "[2022-05-25 11:10:53,727] [INFO] Iteration 25, training loss: 75842.473188\n",
      "[2022-05-25 11:10:54,251] [INFO] Iteration 26, training loss: 68012.754083\n",
      "[2022-05-25 11:10:54,590] [INFO] Iteration 27, training loss: 64764.757938\n",
      "[2022-05-25 11:10:54,843] [INFO] Iteration 28, training loss: 62314.824031\n",
      "[2022-05-25 11:10:55,097] [INFO] Iteration 29, training loss: 58789.802093\n",
      "[2022-05-25 11:10:55,338] [INFO] Iteration 30, training loss: 55223.634864\n",
      "[2022-05-25 11:10:55,608] [INFO] Iteration 31, training loss: 50011.445167\n",
      "[2022-05-25 11:10:55,986] [INFO] Iteration 32, training loss: 47439.593282\n",
      "[2022-05-25 11:10:56,462] [INFO] Iteration 33, training loss: 45114.872455\n",
      "[2022-05-25 11:10:56,942] [INFO] Iteration 34, training loss: 43334.594723\n",
      "[2022-05-25 11:10:57,373] [INFO] Iteration 35, training loss: 40917.298568\n",
      "[2022-05-25 11:10:57,605] [INFO] Iteration 36, training loss: 37403.436755\n",
      "[2022-05-25 11:10:57,844] [INFO] Iteration 37, training loss: 33566.192896\n",
      "[2022-05-25 11:10:58,079] [INFO] Iteration 38, training loss: 30843.560296\n",
      "[2022-05-25 11:10:58,315] [INFO] Iteration 39, training loss: 28610.647722\n",
      "[2022-05-25 11:10:58,551] [INFO] Iteration 40, training loss: 26742.159269\n",
      "[2022-05-25 11:10:58,932] [INFO] Iteration 41, training loss: 25288.324725\n",
      "[2022-05-25 11:10:59,416] [INFO] Iteration 42, training loss: 24124.144925\n",
      "[2022-05-25 11:10:59,922] [INFO] Iteration 43, training loss: 22717.594494\n",
      "[2022-05-25 11:11:00,343] [INFO] Iteration 44, training loss: 21886.827148\n",
      "[2022-05-25 11:11:00,583] [INFO] Iteration 45, training loss: 21315.461487\n",
      "[2022-05-25 11:11:00,832] [INFO] Iteration 46, training loss: 20465.671764\n",
      "[2022-05-25 11:11:01,069] [INFO] Iteration 47, training loss: 19901.275673\n",
      "[2022-05-25 11:11:01,350] [INFO] Iteration 48, training loss: 19355.701731\n",
      "[2022-05-25 11:11:01,597] [INFO] Iteration 49, training loss: 19108.936581\n",
      "[2022-05-25 11:11:01,860] [INFO] Iteration 50, training loss: 18803.919233\n",
      "[2022-05-25 11:11:02,115] [INFO] Iteration 51, training loss: 18646.107274\n",
      "[2022-05-25 11:11:02,351] [INFO] Iteration 52, training loss: 18331.663189\n",
      "[2022-05-25 11:11:02,593] [INFO] Iteration 53, training loss: 18143.902623\n",
      "[2022-05-25 11:11:02,839] [INFO] Iteration 54, training loss: 17957.534276\n",
      "[2022-05-25 11:11:03,109] [INFO] Iteration 55, training loss: 17750.491079\n",
      "[2022-05-25 11:11:03,687] [INFO] Iteration 56, training loss: 17604.372531\n",
      "[2022-05-25 11:11:04,192] [INFO] Iteration 57, training loss: 17497.433552\n",
      "[2022-05-25 11:11:04,671] [INFO] Iteration 58, training loss: 17341.751718\n",
      "[2022-05-25 11:11:05,187] [INFO] Iteration 59, training loss: 17340.657963\n",
      "[2022-05-25 11:11:05,419] [INFO] Iteration 60, training loss: 17243.135538\n",
      "[2022-05-25 11:11:05,675] [INFO] Iteration 61, training loss: 17195.414685\n",
      "[2022-05-25 11:11:05,945] [INFO] Iteration 62, training loss: 17093.046201\n",
      "[2022-05-25 11:11:06,610] [INFO] Iteration 63, training loss: 17092.643496\n",
      "[2022-05-25 11:11:07,209] [INFO] Iteration 64, training loss: 17009.540950\n",
      "[2022-05-25 11:11:07,761] [INFO] Iteration 65, training loss: 16972.071776\n",
      "[2022-05-25 11:11:08,013] [INFO] Iteration 66, training loss: 16900.457502\n",
      "[2022-05-25 11:11:08,289] [INFO] Iteration 67, training loss: 16854.607508\n",
      "[2022-05-25 11:11:08,573] [INFO] Iteration 68, training loss: 16837.219296\n",
      "[2022-05-25 11:11:08,855] [INFO] Iteration 69, training loss: 16822.320255\n",
      "[2022-05-25 11:11:09,117] [INFO] Iteration 70, training loss: 16790.057197\n",
      "[2022-05-25 11:11:09,615] [INFO] Iteration 71, training loss: 16787.026049\n",
      "[2022-05-25 11:11:10,133] [INFO] Iteration 72, training loss: 16711.112918\n",
      "[2022-05-25 11:11:10,628] [INFO] Iteration 73, training loss: 16701.258218\n",
      "[2022-05-25 11:11:10,909] [INFO] Iteration 74, training loss: 16670.085507\n",
      "[2022-05-25 11:11:11,162] [INFO] Iteration 75, training loss: 16660.311446\n",
      "[2022-05-25 11:11:11,438] [INFO] Iteration 76, training loss: 16615.703561\n",
      "[2022-05-25 11:11:11,692] [INFO] Iteration 77, training loss: 16600.224805\n",
      "[2022-05-25 11:11:11,945] [INFO] Iteration 78, training loss: 16566.711124\n",
      "[2022-05-25 11:11:12,217] [INFO] Iteration 79, training loss: 16532.851439\n",
      "[2022-05-25 11:11:12,718] [INFO] Iteration 80, training loss: 16500.962605\n",
      "[2022-05-25 11:11:13,254] [INFO] Iteration 81, training loss: 16480.592373\n",
      "[2022-05-25 11:11:13,768] [INFO] Iteration 82, training loss: 16457.586021\n",
      "[2022-05-25 11:11:14,033] [INFO] Iteration 83, training loss: 16433.056206\n",
      "[2022-05-25 11:11:14,277] [INFO] Iteration 84, training loss: 16416.579562\n",
      "[2022-05-25 11:11:14,536] [INFO] Iteration 85, training loss: 16386.415864\n",
      "[2022-05-25 11:11:15,045] [INFO] Iteration 86, training loss: 16372.890345\n",
      "[2022-05-25 11:11:15,417] [INFO] Iteration 87, training loss: 16362.095627\n",
      "[2022-05-25 11:11:15,979] [INFO] Iteration 88, training loss: 16346.641446\n",
      "[2022-05-25 11:11:16,470] [INFO] Iteration 89, training loss: 16334.886514\n",
      "[2022-05-25 11:11:16,876] [INFO] Iteration 90, training loss: 16323.450120\n",
      "[2022-05-25 11:11:17,122] [INFO] Iteration 91, training loss: 16310.605561\n",
      "[2022-05-25 11:11:17,409] [INFO] Iteration 92, training loss: 16300.375583\n",
      "[2022-05-25 11:11:17,687] [INFO] Iteration 93, training loss: 16289.425327\n",
      "[2022-05-25 11:11:18,055] [INFO] Iteration 94, training loss: 16282.167907\n",
      "[2022-05-25 11:11:18,437] [INFO] Iteration 95, training loss: 16273.891243\n",
      "[2022-05-25 11:11:18,958] [INFO] Iteration 96, training loss: 16266.779198\n",
      "[2022-05-25 11:11:19,461] [INFO] Iteration 97, training loss: 16259.525743\n",
      "[2022-05-25 11:11:19,915] [INFO] Iteration 98, training loss: 16251.731934\n",
      "[2022-05-25 11:11:20,197] [INFO] Iteration 99, training loss: 16247.832453\n",
      "[2022-05-25 11:11:20,467] [INFO] Iteration 100, training loss: 16241.164852\n",
      "[2022-05-25 11:11:20,695] [INFO] Iteration 101, training loss: 16236.047838\n",
      "[2022-05-25 11:11:20,942] [INFO] Iteration 102, training loss: 16229.449792\n",
      "[2022-05-25 11:11:21,220] [INFO] Iteration 103, training loss: 16225.793452\n",
      "[2022-05-25 11:11:21,714] [INFO] Iteration 104, training loss: 16220.116846\n",
      "[2022-05-25 11:11:22,208] [INFO] Iteration 105, training loss: 16217.221825\n",
      "[2022-05-25 11:11:22,710] [INFO] Iteration 106, training loss: 16212.566072\n",
      "[2022-05-25 11:11:23,009] [INFO] Iteration 107, training loss: 16209.496987\n",
      "[2022-05-25 11:11:23,267] [INFO] Iteration 108, training loss: 16204.219403\n",
      "[2022-05-25 11:11:23,531] [INFO] Iteration 109, training loss: 16202.327269\n",
      "[2022-05-25 11:11:23,792] [INFO] Iteration 110, training loss: 16198.259803\n",
      "[2022-05-25 11:11:24,070] [INFO] Iteration 111, training loss: 16196.294216\n",
      "[2022-05-25 11:11:24,585] [INFO] Iteration 112, training loss: 16192.818622\n",
      "[2022-05-25 11:11:25,127] [INFO] Iteration 113, training loss: 16190.807656\n",
      "[2022-05-25 11:11:25,595] [INFO] Iteration 114, training loss: 16187.622113\n",
      "[2022-05-25 11:11:25,907] [INFO] Iteration 115, training loss: 16186.063478\n",
      "[2022-05-25 11:11:26,150] [INFO] Iteration 116, training loss: 16183.325785\n",
      "[2022-05-25 11:11:26,389] [INFO] Iteration 117, training loss: 16181.955672\n",
      "[2022-05-25 11:11:26,624] [INFO] Iteration 118, training loss: 16179.522570\n",
      "[2022-05-25 11:11:26,866] [INFO] Iteration 119, training loss: 16176.955902\n",
      "[2022-05-25 11:11:27,113] [INFO] Iteration 120, training loss: 16174.186865\n",
      "[2022-05-25 11:11:27,503] [INFO] Iteration 121, training loss: 16173.204877\n",
      "[2022-05-25 11:11:27,967] [INFO] Iteration 122, training loss: 16170.687800\n",
      "[2022-05-25 11:11:28,563] [INFO] Iteration 123, training loss: 16168.023697\n",
      "[2022-05-25 11:11:28,930] [INFO] Iteration 124, training loss: 16165.885510\n",
      "[2022-05-25 11:11:29,182] [INFO] Iteration 125, training loss: 16163.362532\n",
      "[2022-05-25 11:11:29,425] [INFO] Iteration 126, training loss: 16160.359490\n",
      "[2022-05-25 11:11:29,667] [INFO] Iteration 127, training loss: 16159.116075\n",
      "[2022-05-25 11:11:29,899] [INFO] Iteration 128, training loss: 16156.611033\n",
      "[2022-05-25 11:11:30,135] [INFO] Iteration 129, training loss: 16154.696393\n",
      "[2022-05-25 11:11:30,613] [INFO] Iteration 130, training loss: 16151.463198\n",
      "[2022-05-25 11:11:31,098] [INFO] Iteration 131, training loss: 16150.244786\n",
      "[2022-05-25 11:11:31,600] [INFO] Iteration 132, training loss: 16147.862274\n",
      "[2022-05-25 11:11:31,917] [INFO] Iteration 133, training loss: 16146.116513\n",
      "[2022-05-25 11:11:32,156] [INFO] Iteration 134, training loss: 16144.107791\n",
      "[2022-05-25 11:11:32,405] [INFO] Iteration 135, training loss: 16141.972632\n",
      "[2022-05-25 11:11:32,650] [INFO] Iteration 136, training loss: 16139.468005\n",
      "[2022-05-25 11:11:32,884] [INFO] Iteration 137, training loss: 16137.185846\n",
      "[2022-05-25 11:11:33,118] [INFO] Iteration 138, training loss: 16135.535118\n",
      "[2022-05-25 11:11:33,459] [INFO] Iteration 139, training loss: 16133.390675\n",
      "[2022-05-25 11:11:33,923] [INFO] Iteration 140, training loss: 16131.707915\n",
      "[2022-05-25 11:11:34,387] [INFO] Iteration 141, training loss: 16129.519305\n",
      "[2022-05-25 11:11:34,855] [INFO] Iteration 142, training loss: 16128.076988\n",
      "[2022-05-25 11:11:35,085] [INFO] Iteration 143, training loss: 16126.042172\n",
      "[2022-05-25 11:11:35,315] [INFO] Iteration 144, training loss: 16124.793203\n",
      "[2022-05-25 11:11:35,545] [INFO] Iteration 145, training loss: 16123.137805\n",
      "[2022-05-25 11:11:35,775] [INFO] Iteration 146, training loss: 16121.958769\n",
      "[2022-05-25 11:11:36,006] [INFO] Iteration 147, training loss: 16120.250376\n",
      "[2022-05-25 11:11:36,281] [INFO] Iteration 148, training loss: 16118.996159\n",
      "[2022-05-25 11:11:36,741] [INFO] Iteration 149, training loss: 16117.485176\n",
      "[2022-05-25 11:11:37,204] [INFO] Iteration 150, training loss: 16116.026683\n",
      "[2022-05-25 11:11:37,665] [INFO] Iteration 151, training loss: 16114.215267\n",
      "[2022-05-25 11:11:37,957] [INFO] Iteration 152, training loss: 16112.814582\n",
      "[2022-05-25 11:11:38,187] [INFO] Iteration 153, training loss: 16111.390885\n",
      "[2022-05-25 11:11:38,417] [INFO] Iteration 154, training loss: 16110.229082\n",
      "[2022-05-25 11:11:38,686] [INFO] Iteration 155, training loss: 16108.276302\n",
      "[2022-05-25 11:11:38,920] [INFO] Iteration 156, training loss: 16106.515369\n",
      "[2022-05-25 11:11:39,193] [INFO] Iteration 157, training loss: 16105.075836\n",
      "[2022-05-25 11:11:39,659] [INFO] Iteration 158, training loss: 16103.524775\n",
      "[2022-05-25 11:11:40,122] [INFO] Iteration 159, training loss: 16102.039520\n",
      "[2022-05-25 11:11:40,624] [INFO] Iteration 160, training loss: 16100.822599\n",
      "[2022-05-25 11:11:40,938] [INFO] Iteration 161, training loss: 16099.466799\n",
      "[2022-05-25 11:11:41,184] [INFO] Iteration 162, training loss: 16097.523403\n",
      "[2022-05-25 11:11:41,433] [INFO] Iteration 163, training loss: 16097.064868\n",
      "[2022-05-25 11:11:41,679] [INFO] Iteration 164, training loss: 16095.784258\n",
      "[2022-05-25 11:11:41,928] [INFO] Iteration 165, training loss: 16094.195110\n",
      "[2022-05-25 11:11:42,181] [INFO] Iteration 166, training loss: 16093.030758\n",
      "[2022-05-25 11:11:42,640] [INFO] Iteration 167, training loss: 16091.249820\n",
      "[2022-05-25 11:11:43,159] [INFO] Iteration 168, training loss: 16090.317491\n",
      "[2022-05-25 11:11:43,697] [INFO] Iteration 169, training loss: 16088.858646\n",
      "[2022-05-25 11:11:44,044] [INFO] Iteration 170, training loss: 16088.326393\n",
      "[2022-05-25 11:11:44,279] [INFO] Iteration 171, training loss: 16086.235056\n",
      "[2022-05-25 11:11:44,516] [INFO] Iteration 172, training loss: 16085.352039\n",
      "[2022-05-25 11:11:44,747] [INFO] Iteration 173, training loss: 16083.636954\n",
      "[2022-05-25 11:11:44,980] [INFO] Iteration 174, training loss: 16083.106583\n",
      "[2022-05-25 11:11:45,258] [INFO] Iteration 175, training loss: 16081.574570\n",
      "[2022-05-25 11:11:45,807] [INFO] Iteration 176, training loss: 16080.818812\n",
      "[2022-05-25 11:11:46,365] [INFO] Iteration 177, training loss: 16079.354065\n",
      "[2022-05-25 11:11:46,821] [INFO] Iteration 178, training loss: 16078.542032\n",
      "[2022-05-25 11:11:47,075] [INFO] Iteration 179, training loss: 16077.231962\n",
      "[2022-05-25 11:11:47,348] [INFO] Iteration 180, training loss: 16076.592644\n",
      "[2022-05-25 11:11:47,627] [INFO] Iteration 181, training loss: 16075.247808\n",
      "[2022-05-25 11:11:47,872] [INFO] Iteration 182, training loss: 16074.089988\n",
      "[2022-05-25 11:11:48,112] [INFO] Iteration 183, training loss: 16072.644745\n",
      "[2022-05-25 11:11:48,500] [INFO] Iteration 184, training loss: 16071.262353\n",
      "[2022-05-25 11:11:48,991] [INFO] Iteration 185, training loss: 16069.792235\n",
      "[2022-05-25 11:11:49,519] [INFO] Iteration 186, training loss: 16068.410221\n",
      "[2022-05-25 11:11:49,947] [INFO] Iteration 187, training loss: 16067.163593\n",
      "[2022-05-25 11:11:50,203] [INFO] Iteration 188, training loss: 16065.242429\n",
      "[2022-05-25 11:11:50,447] [INFO] Iteration 189, training loss: 16063.882382\n",
      "[2022-05-25 11:11:50,704] [INFO] Iteration 190, training loss: 16061.620476\n",
      "[2022-05-25 11:11:50,979] [INFO] Iteration 191, training loss: 16060.515738\n",
      "[2022-05-25 11:11:51,260] [INFO] Iteration 192, training loss: 16058.300741\n",
      "[2022-05-25 11:11:51,745] [INFO] Iteration 193, training loss: 16057.028734\n",
      "[2022-05-25 11:11:52,223] [INFO] Iteration 194, training loss: 16054.706491\n",
      "[2022-05-25 11:11:52,700] [INFO] Iteration 195, training loss: 16053.582598\n",
      "[2022-05-25 11:11:52,986] [INFO] Iteration 196, training loss: 16051.452814\n",
      "[2022-05-25 11:11:53,231] [INFO] Iteration 197, training loss: 16050.334590\n",
      "[2022-05-25 11:11:53,491] [INFO] Iteration 198, training loss: 16048.608484\n",
      "[2022-05-25 11:11:53,734] [INFO] Iteration 199, training loss: 16047.133438\n",
      "[2022-05-25 11:11:53,968] [INFO] Iteration 200, training loss: 16045.390393\n",
      "[2022-05-25 11:11:54,287] [INFO] Iteration 201, training loss: 16044.027686\n",
      "[2022-05-25 11:11:54,767] [INFO] Iteration 202, training loss: 16042.079700\n",
      "[2022-05-25 11:11:55,233] [INFO] Iteration 203, training loss: 16040.807591\n",
      "[2022-05-25 11:11:55,724] [INFO] Iteration 204, training loss: 16039.354837\n",
      "[2022-05-25 11:11:55,961] [INFO] Iteration 205, training loss: 16037.845145\n",
      "[2022-05-25 11:11:56,195] [INFO] Iteration 206, training loss: 16036.714353\n",
      "[2022-05-25 11:11:56,438] [INFO] Iteration 207, training loss: 16034.999270\n",
      "[2022-05-25 11:11:56,685] [INFO] Iteration 208, training loss: 16034.229970\n",
      "[2022-05-25 11:11:56,951] [INFO] Iteration 209, training loss: 16032.791409\n",
      "[2022-05-25 11:11:57,198] [INFO] Iteration 210, training loss: 16031.770920\n",
      "[2022-05-25 11:11:57,692] [INFO] Iteration 211, training loss: 16029.886456\n",
      "[2022-05-25 11:11:58,195] [INFO] Iteration 212, training loss: 16028.499115\n",
      "[2022-05-25 11:11:58,704] [INFO] Iteration 213, training loss: 16027.220048\n",
      "[2022-05-25 11:11:59,024] [INFO] Iteration 214, training loss: 16025.731148\n",
      "[2022-05-25 11:11:59,286] [INFO] Iteration 215, training loss: 16024.217826\n",
      "[2022-05-25 11:11:59,527] [INFO] Iteration 216, training loss: 16022.483510\n",
      "[2022-05-25 11:11:59,774] [INFO] Iteration 217, training loss: 16021.088998\n",
      "[2022-05-25 11:12:00,028] [INFO] Iteration 218, training loss: 16019.566974\n",
      "[2022-05-25 11:12:00,421] [INFO] Iteration 219, training loss: 16018.074439\n",
      "[2022-05-25 11:12:00,936] [INFO] Iteration 220, training loss: 16016.349677\n",
      "[2022-05-25 11:12:01,466] [INFO] Iteration 221, training loss: 16015.042547\n",
      "[2022-05-25 11:12:01,888] [INFO] Iteration 222, training loss: 16013.364620\n",
      "[2022-05-25 11:12:02,155] [INFO] Iteration 223, training loss: 16010.716466\n",
      "[2022-05-25 11:12:02,441] [INFO] Iteration 224, training loss: 16010.116564\n",
      "[2022-05-25 11:12:02,679] [INFO] Iteration 225, training loss: 16007.955863\n",
      "[2022-05-25 11:12:02,948] [INFO] Iteration 226, training loss: 16006.506357\n",
      "[2022-05-25 11:12:03,204] [INFO] Iteration 227, training loss: 16004.629434\n",
      "[2022-05-25 11:12:03,737] [INFO] Iteration 228, training loss: 16003.236044\n",
      "[2022-05-25 11:12:04,204] [INFO] Iteration 229, training loss: 16001.747536\n",
      "[2022-05-25 11:12:04,692] [INFO] Iteration 230, training loss: 16000.233494\n",
      "[2022-05-25 11:12:05,067] [INFO] Iteration 231, training loss: 15999.037480\n",
      "[2022-05-25 11:12:05,367] [INFO] Iteration 232, training loss: 15997.505407\n",
      "[2022-05-25 11:12:05,607] [INFO] Iteration 233, training loss: 15996.409494\n",
      "[2022-05-25 11:12:05,848] [INFO] Iteration 234, training loss: 15995.068001\n",
      "[2022-05-25 11:12:06,091] [INFO] Iteration 235, training loss: 15994.169300\n",
      "[2022-05-25 11:12:06,519] [INFO] Iteration 236, training loss: 15992.757029\n",
      "[2022-05-25 11:12:07,041] [INFO] Iteration 237, training loss: 15992.011990\n",
      "[2022-05-25 11:12:07,618] [INFO] Iteration 238, training loss: 15990.170182\n",
      "[2022-05-25 11:12:08,002] [INFO] Iteration 239, training loss: 15988.520586\n",
      "[2022-05-25 11:12:08,296] [INFO] Iteration 240, training loss: 15986.815254\n",
      "[2022-05-25 11:12:08,534] [INFO] Iteration 241, training loss: 15986.510709\n",
      "[2022-05-25 11:12:08,772] [INFO] Iteration 242, training loss: 15983.652645\n",
      "[2022-05-25 11:12:09,019] [INFO] Iteration 243, training loss: 15981.683481\n",
      "[2022-05-25 11:12:09,307] [INFO] Iteration 244, training loss: 15979.753516\n",
      "[2022-05-25 11:12:09,625] [INFO] Iteration 245, training loss: 15978.152410\n",
      "[2022-05-25 11:12:09,939] [INFO] Iteration 246, training loss: 15976.496280\n",
      "[2022-05-25 11:12:10,246] [INFO] Iteration 247, training loss: 15975.191376\n",
      "[2022-05-25 11:12:10,557] [INFO] Iteration 248, training loss: 15973.710700\n",
      "[2022-05-25 11:12:10,898] [INFO] Iteration 249, training loss: 15972.452935\n",
      "[2022-05-25 11:12:11,409] [INFO] Iteration 250, training loss: 15970.944863\n",
      "[2022-05-25 11:12:11,919] [INFO] Iteration 251, training loss: 15969.265381\n",
      "[2022-05-25 11:12:12,406] [INFO] Iteration 252, training loss: 15967.854511\n",
      "[2022-05-25 11:12:12,679] [INFO] Iteration 253, training loss: 15966.512207\n",
      "[2022-05-25 11:12:12,953] [INFO] Iteration 254, training loss: 15966.123713\n",
      "[2022-05-25 11:12:13,264] [INFO] Iteration 255, training loss: 15964.258901\n",
      "[2022-05-25 11:12:13,558] [INFO] Iteration 256, training loss: 15961.617960\n",
      "[2022-05-25 11:12:13,923] [INFO] Iteration 257, training loss: 15960.058842\n",
      "[2022-05-25 11:12:14,443] [INFO] Iteration 258, training loss: 15958.756236\n",
      "[2022-05-25 11:12:14,948] [INFO] Iteration 259, training loss: 15956.748532\n",
      "[2022-05-25 11:12:15,368] [INFO] Iteration 260, training loss: 15955.726445\n",
      "[2022-05-25 11:12:15,616] [INFO] Iteration 261, training loss: 15953.544297\n",
      "[2022-05-25 11:12:15,844] [INFO] Iteration 262, training loss: 15953.202867\n",
      "[2022-05-25 11:12:16,080] [INFO] Iteration 263, training loss: 15950.784691\n",
      "[2022-05-25 11:12:16,320] [INFO] Iteration 264, training loss: 15949.491400\n",
      "[2022-05-25 11:12:16,568] [INFO] Iteration 265, training loss: 15948.351022\n",
      "[2022-05-25 11:12:16,953] [INFO] Iteration 266, training loss: 15947.970308\n",
      "[2022-05-25 11:12:17,462] [INFO] Iteration 267, training loss: 15945.165421\n",
      "[2022-05-25 11:12:17,957] [INFO] Iteration 268, training loss: 15944.410272\n",
      "[2022-05-25 11:12:18,358] [INFO] Iteration 269, training loss: 15942.318103\n",
      "[2022-05-25 11:12:18,650] [INFO] Iteration 270, training loss: 15941.328155\n",
      "[2022-05-25 11:12:18,909] [INFO] Iteration 271, training loss: 15939.837577\n",
      "[2022-05-25 11:12:19,160] [INFO] Iteration 272, training loss: 15938.791373\n",
      "[2022-05-25 11:12:19,391] [INFO] Iteration 273, training loss: 15937.538440\n",
      "[2022-05-25 11:12:19,625] [INFO] Iteration 274, training loss: 15936.119289\n",
      "[2022-05-25 11:12:19,969] [INFO] Iteration 275, training loss: 15935.164146\n",
      "[2022-05-25 11:12:20,479] [INFO] Iteration 276, training loss: 15933.910733\n",
      "[2022-05-25 11:12:20,960] [INFO] Iteration 277, training loss: 15933.174949\n",
      "[2022-05-25 11:12:21,389] [INFO] Iteration 278, training loss: 15932.094664\n",
      "[2022-05-25 11:12:21,633] [INFO] Iteration 279, training loss: 15931.261373\n",
      "[2022-05-25 11:12:21,890] [INFO] Iteration 280, training loss: 15930.072220\n",
      "[2022-05-25 11:12:22,141] [INFO] Iteration 281, training loss: 15929.422778\n",
      "[2022-05-25 11:12:22,409] [INFO] Iteration 282, training loss: 15928.345593\n",
      "[2022-05-25 11:12:22,650] [INFO] Iteration 283, training loss: 15927.626430\n",
      "[2022-05-25 11:12:23,110] [INFO] Iteration 284, training loss: 15926.586786\n",
      "[2022-05-25 11:12:23,599] [INFO] Iteration 285, training loss: 15925.782994\n",
      "[2022-05-25 11:12:24,084] [INFO] Iteration 286, training loss: 15925.038118\n",
      "[2022-05-25 11:12:24,436] [INFO] Iteration 287, training loss: 15924.102346\n",
      "[2022-05-25 11:12:24,671] [INFO] Iteration 288, training loss: 15923.626276\n",
      "[2022-05-25 11:12:24,901] [INFO] Iteration 289, training loss: 15922.735057\n",
      "[2022-05-25 11:12:25,132] [INFO] Iteration 290, training loss: 15922.266867\n",
      "[2022-05-25 11:12:25,361] [INFO] Iteration 291, training loss: 15921.451611\n",
      "[2022-05-25 11:12:25,591] [INFO] Iteration 292, training loss: 15920.885878\n",
      "[2022-05-25 11:12:25,820] [INFO] Iteration 293, training loss: 15919.883453\n",
      "[2022-05-25 11:12:26,072] [INFO] Iteration 294, training loss: 15919.105341\n",
      "[2022-05-25 11:12:26,308] [INFO] Iteration 295, training loss: 15918.347671\n",
      "[2022-05-25 11:12:26,534] [INFO] Iteration 296, training loss: 15917.584495\n",
      "[2022-05-25 11:12:26,763] [INFO] Iteration 297, training loss: 15916.729575\n",
      "[2022-05-25 11:12:26,992] [INFO] Iteration 298, training loss: 15916.078679\n",
      "[2022-05-25 11:12:27,221] [INFO] Iteration 299, training loss: 15915.453836\n",
      "[2022-05-25 11:12:27,656] [INFO] Iteration 300, training loss: 15915.045246\n",
      "[2022-05-25 11:12:28,156] [INFO] Iteration 301, training loss: 15914.257047\n",
      "[2022-05-25 11:12:28,664] [INFO] Iteration 302, training loss: 15913.625050\n",
      "[2022-05-25 11:12:29,002] [INFO] Iteration 303, training loss: 15913.040965\n",
      "[2022-05-25 11:12:29,235] [INFO] Iteration 304, training loss: 15912.392620\n",
      "[2022-05-25 11:12:29,467] [INFO] Iteration 305, training loss: 15911.433208\n",
      "[2022-05-25 11:12:29,697] [INFO] Iteration 306, training loss: 15910.946324\n",
      "[2022-05-25 11:12:29,933] [INFO] Iteration 307, training loss: 15909.998643\n",
      "[2022-05-25 11:12:30,164] [INFO] Iteration 308, training loss: 15909.418334\n",
      "[2022-05-25 11:12:30,586] [INFO] Iteration 309, training loss: 15908.769048\n",
      "[2022-05-25 11:12:31,047] [INFO] Iteration 310, training loss: 15908.278537\n",
      "[2022-05-25 11:12:31,513] [INFO] Iteration 311, training loss: 15907.283044\n",
      "[2022-05-25 11:12:31,906] [INFO] Iteration 312, training loss: 15906.551210\n",
      "[2022-05-25 11:12:32,143] [INFO] Iteration 313, training loss: 15905.722173\n",
      "[2022-05-25 11:12:32,397] [INFO] Iteration 314, training loss: 15905.041109\n",
      "[2022-05-25 11:12:32,641] [INFO] Iteration 315, training loss: 15904.277231\n",
      "[2022-05-25 11:12:32,887] [INFO] Iteration 316, training loss: 15903.451508\n",
      "[2022-05-25 11:12:33,121] [INFO] Iteration 317, training loss: 15902.896182\n",
      "[2022-05-25 11:12:33,578] [INFO] Iteration 318, training loss: 15902.083112\n",
      "[2022-05-25 11:12:34,080] [INFO] Iteration 319, training loss: 15901.583839\n",
      "[2022-05-25 11:12:34,568] [INFO] Iteration 320, training loss: 15900.853213\n",
      "[2022-05-25 11:12:34,895] [INFO] Iteration 321, training loss: 15900.307778\n",
      "[2022-05-25 11:12:35,146] [INFO] Iteration 322, training loss: 15899.544023\n",
      "[2022-05-25 11:12:35,382] [INFO] Iteration 323, training loss: 15898.924543\n",
      "[2022-05-25 11:12:35,662] [INFO] Iteration 324, training loss: 15898.237670\n",
      "[2022-05-25 11:12:35,911] [INFO] Iteration 325, training loss: 15897.476502\n",
      "[2022-05-25 11:12:36,153] [INFO] Iteration 326, training loss: 15896.924952\n",
      "[2022-05-25 11:12:36,402] [INFO] Iteration 327, training loss: 15896.019203\n",
      "[2022-05-25 11:12:36,631] [INFO] Iteration 328, training loss: 15895.210599\n",
      "[2022-05-25 11:12:36,880] [INFO] Iteration 329, training loss: 15894.517776\n",
      "[2022-05-25 11:12:37,117] [INFO] Iteration 330, training loss: 15893.796477\n",
      "[2022-05-25 11:12:37,353] [INFO] Iteration 331, training loss: 15893.038794\n",
      "[2022-05-25 11:12:37,582] [INFO] Iteration 332, training loss: 15892.167991\n",
      "[2022-05-25 11:12:37,823] [INFO] Iteration 333, training loss: 15891.387023\n",
      "[2022-05-25 11:12:38,107] [INFO] Iteration 334, training loss: 15890.588742\n",
      "[2022-05-25 11:12:38,371] [INFO] Iteration 335, training loss: 15889.826479\n",
      "[2022-05-25 11:12:38,616] [INFO] Iteration 336, training loss: 15889.122733\n",
      "[2022-05-25 11:12:38,845] [INFO] Iteration 337, training loss: 15888.348937\n",
      "[2022-05-25 11:12:39,079] [INFO] Iteration 338, training loss: 15887.575483\n",
      "[2022-05-25 11:12:39,421] [INFO] Iteration 339, training loss: 15886.804180\n",
      "[2022-05-25 11:12:39,891] [INFO] Iteration 340, training loss: 15886.066320\n",
      "[2022-05-25 11:12:40,353] [INFO] Iteration 341, training loss: 15885.376614\n",
      "[2022-05-25 11:12:40,813] [INFO] Iteration 342, training loss: 15884.629274\n",
      "[2022-05-25 11:12:41,054] [INFO] Iteration 343, training loss: 15883.936029\n",
      "[2022-05-25 11:12:41,289] [INFO] Iteration 344, training loss: 15883.129301\n",
      "[2022-05-25 11:12:41,539] [INFO] Iteration 345, training loss: 15882.643871\n",
      "[2022-05-25 11:12:41,800] [INFO] Iteration 346, training loss: 15881.872996\n",
      "[2022-05-25 11:12:42,052] [INFO] Iteration 347, training loss: 15881.236166\n",
      "[2022-05-25 11:12:42,430] [INFO] Iteration 348, training loss: 15880.633027\n",
      "[2022-05-25 11:12:42,931] [INFO] Iteration 349, training loss: 15880.159816\n",
      "[2022-05-25 11:12:43,409] [INFO] Iteration 350, training loss: 15879.308921\n",
      "[2022-05-25 11:12:43,818] [INFO] Iteration 351, training loss: 15878.756788\n",
      "[2022-05-25 11:12:44,061] [INFO] Iteration 352, training loss: 15877.956870\n",
      "[2022-05-25 11:12:44,310] [INFO] Iteration 353, training loss: 15877.494687\n",
      "[2022-05-25 11:12:44,544] [INFO] Iteration 354, training loss: 15876.671520\n",
      "[2022-05-25 11:12:44,794] [INFO] Iteration 355, training loss: 15876.254035\n",
      "[2022-05-25 11:12:45,042] [INFO] Iteration 356, training loss: 15875.500097\n",
      "[2022-05-25 11:12:45,322] [INFO] Iteration 357, training loss: 15874.989559\n",
      "[2022-05-25 11:12:45,806] [INFO] Iteration 358, training loss: 15874.346585\n",
      "[2022-05-25 11:12:46,278] [INFO] Iteration 359, training loss: 15873.863443\n",
      "[2022-05-25 11:12:46,737] [INFO] Iteration 360, training loss: 15873.252417\n",
      "[2022-05-25 11:12:47,055] [INFO] Iteration 361, training loss: 15872.831775\n",
      "[2022-05-25 11:12:47,315] [INFO] Iteration 362, training loss: 15872.257303\n",
      "[2022-05-25 11:12:47,568] [INFO] Iteration 363, training loss: 15871.862222\n",
      "[2022-05-25 11:12:47,807] [INFO] Iteration 364, training loss: 15871.425272\n",
      "[2022-05-25 11:12:48,097] [INFO] Iteration 365, training loss: 15871.032098\n",
      "[2022-05-25 11:12:48,510] [INFO] Iteration 366, training loss: 15870.677433\n",
      "[2022-05-25 11:12:48,999] [INFO] Iteration 367, training loss: 15870.325483\n",
      "[2022-05-25 11:12:49,499] [INFO] Iteration 368, training loss: 15870.073440\n",
      "[2022-05-25 11:12:49,877] [INFO] Iteration 369, training loss: 15869.663813\n",
      "[2022-05-25 11:12:50,115] [INFO] Iteration 370, training loss: 15869.481921\n",
      "[2022-05-25 11:12:50,344] [INFO] Iteration 371, training loss: 15868.824114\n",
      "[2022-05-25 11:12:50,573] [INFO] Iteration 372, training loss: 15868.676411\n",
      "[2022-05-25 11:12:50,801] [INFO] Iteration 373, training loss: 15868.076115\n",
      "[2022-05-25 11:12:51,041] [INFO] Iteration 374, training loss: 15867.891314\n",
      "[2022-05-25 11:12:51,269] [INFO] Iteration 375, training loss: 15867.282625\n",
      "[2022-05-25 11:12:51,757] [INFO] Iteration 376, training loss: 15867.202255\n",
      "[2022-05-25 11:12:52,292] [INFO] Iteration 377, training loss: 15866.618565\n",
      "[2022-05-25 11:12:52,787] [INFO] Iteration 378, training loss: 15866.492524\n",
      "[2022-05-25 11:12:53,065] [INFO] Iteration 379, training loss: 15866.017816\n",
      "[2022-05-25 11:12:53,314] [INFO] Iteration 380, training loss: 15865.908283\n",
      "[2022-05-25 11:12:53,321] [INFO] Terminated with the stopping criteria\n",
      "[2022-05-25 11:12:53,321] [INFO] Saving model\n"
     ]
    }
   ],
   "source": [
    "model = chaine.train(train_sentences, train_labels, verbose=0, optimize_hyperparameters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c409a14e-143d-4843-9b99-d4002e4e999b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.85      0.79      0.82      1668\n",
      "        MISC       0.80      0.70      0.75       702\n",
      "         ORG       0.77      0.64      0.70      1661\n",
      "         PER       0.80      0.84      0.82      1617\n",
      "\n",
      "   micro avg       0.81      0.75      0.78      5648\n",
      "   macro avg       0.80      0.74      0.77      5648\n",
      "weighted avg       0.81      0.75      0.77      5648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_sentences)\n",
    "\n",
    "print(classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a80185a-1546-463a-9795-f49bb43a2ba5",
   "metadata": {},
   "source": [
    "## Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47c4eebc-d32d-4a5c-a452-caaff999a8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>5.480503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>5.142529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>I-ORG</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>5.054006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>B-PER</td>\n",
       "      <td>I-PER</td>\n",
       "      <td>4.621046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>B-MISC</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>4.454587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>O</td>\n",
       "      <td>B-MISC</td>\n",
       "      <td>4.309001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>I-MISC</td>\n",
       "      <td>I-MISC</td>\n",
       "      <td>4.072832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>O</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>3.984640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>B-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>3.686581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>I-LOC</td>\n",
       "      <td>I-LOC</td>\n",
       "      <td>3.629553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      from      to    weight\n",
       "6    B-ORG   I-ORG  5.480503\n",
       "10       O       O  5.142529\n",
       "60   I-ORG   I-ORG  5.054006\n",
       "31   B-PER   I-PER  4.621046\n",
       "25  B-MISC  I-MISC  4.454587\n",
       "11       O  B-MISC  4.309001\n",
       "70  I-MISC  I-MISC  4.072832\n",
       "12       O   B-PER  3.984640\n",
       "53   B-LOC   I-LOC  3.686581\n",
       "80   I-LOC   I-LOC  3.629553"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transitions = pd.DataFrame(model.transitions)\n",
    "transitions.sort_values(\"weight\", ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "981bdfd2-65d5-4c2f-a3ba-6e351118d246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>label</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>EOS</td>\n",
       "      <td>O</td>\n",
       "      <td>4.452106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>BOS</td>\n",
       "      <td>O</td>\n",
       "      <td>3.263974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>token[-3:]:day</td>\n",
       "      <td>O</td>\n",
       "      <td>2.879297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142519</th>\n",
       "      <td>token[-2:]:5M</td>\n",
       "      <td>O</td>\n",
       "      <td>2.856290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142632</th>\n",
       "      <td>token[-2:]:0M</td>\n",
       "      <td>O</td>\n",
       "      <td>2.812777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39882</th>\n",
       "      <td>-1:token.lower():v</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>2.672666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4651</th>\n",
       "      <td>-1:token.lower():at</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>2.664489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31888</th>\n",
       "      <td>token[-2:]:I</td>\n",
       "      <td>O</td>\n",
       "      <td>2.658717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31879</th>\n",
       "      <td>token[-3:]:I</td>\n",
       "      <td>O</td>\n",
       "      <td>2.658717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76909</th>\n",
       "      <td>token.lower():clinton</td>\n",
       "      <td>B-PER</td>\n",
       "      <td>2.546819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      feature  label    weight\n",
       "405                       EOS      O  4.452106\n",
       "58                        BOS      O  3.263974\n",
       "751            token[-3:]:day      O  2.879297\n",
       "142519          token[-2:]:5M      O  2.856290\n",
       "142632          token[-2:]:0M      O  2.812777\n",
       "39882      -1:token.lower():v  B-ORG  2.672666\n",
       "4651      -1:token.lower():at  B-LOC  2.664489\n",
       "31888            token[-2:]:I      O  2.658717\n",
       "31879            token[-3:]:I      O  2.658717\n",
       "76909   token.lower():clinton  B-PER  2.546819"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states = pd.DataFrame(model.states)\n",
    "states.sort_values(\"weight\", ascending=False)[:10]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b6305eb58051137afc5a5205478d9ee79d6169039fed1d5a046194240369b06c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
