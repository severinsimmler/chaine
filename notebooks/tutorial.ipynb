{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named entity recognition with `chaine`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chaine\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"germeval_14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence: ['Schartau', 'sagte', 'dem', '\"', 'Tagesspiegel', '\"', 'vom', 'Freitag', ',', 'Fischer', 'sei', '\"', 'in', 'einer', 'Weise', 'aufgetreten', ',', 'die', 'alles', 'andere', 'als', 'überzeugend', 'war', '\"', '.']\n",
      "Labels: [19, 0, 0, 0, 7, 0, 0, 0, 0, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sequence: {dataset['train']['tokens'][0]}\")\n",
    "print(f\"Labels: {dataset['train']['ner_tags'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform to proper sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sequences = chaine.token_sequences(dataset[\"train\"][\"tokens\"])\n",
    "label_sequences = chaine.label_sequences(dataset[\"train\"][\"ner_tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TokenSequence: [<Token 0: Schartau>, <Token 1: sagte>, <Token 2: dem>, <Token 3: \">, <Token 4: Tagesspiegel>, <Token 5: \">, <Token 6: vom>, <Token 7: Freitag>, <Token 8: ,>, <Token 9: Fischer>, <Token 10: sei>, <Token 11: \">, <Token 12: in>, <Token 13: einer>, <Token 14: Weise>, <Token 15: aufgetreten>, <Token 16: ,>, <Token 17: die>, <Token 18: alles>, <Token 19: andere>, <Token 20: als>, <Token 21: überzeugend>, <Token 22: war>, <Token 23: \">, <Token 24: .>]>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(token_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<LabelSequence: ['19', '0', '0', '0', '7', '0', '0', '0', '0', '19', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(label_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model using the high-level API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-11-29 22:48:56,977] [INFO] Loading data\n",
      "[2020-11-29 22:49:05,301] [INFO] Start training\n",
      "[2020-11-29 22:49:08,678] [INFO] Iteration: 1\tLoss: 549913.612197\n",
      "[2020-11-29 22:49:09,796] [INFO] Iteration: 2\tLoss: 377078.992349\n",
      "[2020-11-29 22:49:13,021] [INFO] Iteration: 3\tLoss: 267322.682539\n",
      "[2020-11-29 22:49:15,353] [INFO] Iteration: 4\tLoss: 248339.616374\n",
      "[2020-11-29 22:49:17,698] [INFO] Iteration: 5\tLoss: 233494.412198\n",
      "[2020-11-29 22:49:18,755] [INFO] Iteration: 6\tLoss: 223343.503914\n",
      "[2020-11-29 22:49:19,795] [INFO] Iteration: 7\tLoss: 208772.524234\n",
      "[2020-11-29 22:49:20,923] [INFO] Iteration: 8\tLoss: 190119.766701\n",
      "[2020-11-29 22:49:22,212] [INFO] Iteration: 9\tLoss: 178851.893751\n",
      "[2020-11-29 22:49:23,356] [INFO] Iteration: 10\tLoss: 170839.004304\n"
     ]
    }
   ],
   "source": [
    "crf = chaine.train(token_sequences, label_sequences, max_iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sequence = next(chaine.token_sequences(dataset[\"train\"][\"tokens\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf.predict(token_sequence.featurize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom features and lower-level training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from chaine.data import Token, TokenSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function featurize in module chaine.data:\n",
      "\n",
      "featurize(self) -> Generator[List[str], NoneType, NoneType]\n",
      "    Extract features from tokens of the sequence\n",
      "    \n",
      "    Note\n",
      "    ----\n",
      "    Overwrite this method for custom feature extraction – which is generally\n",
      "    recommended as the default features may result in very low accuracy.\n",
      "    \n",
      "    One token is represented as a set of strings, each string is a unique feature,\n",
      "    e.g. the string representation of the current token.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(TokenSequence.featurize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'de'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-fcb272fc0ff6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'de'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens_from_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'I'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'like'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cookies'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Do'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'you'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'?'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/chaine/.env/lib/python3.8/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/chaine/.env/lib/python3.8/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'de'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('de')\n",
    "nlp.tokenizer = nlp.tokenizer.tokens_from_list\n",
    "\n",
    "for doc in nlp.pipe([['I', 'like', 'cookies', '.'], ['Do', 'you', '?']]):\n",
    "    for token in doc:\n",
    "        print(token, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenSequence(TokenSequence):\n",
    "    def __post_init__(self):\n",
    "        self.nlp = spacy.load(\"de_core_news_sm\")\n",
    "        self.nlp.tokenizer = self.nlp.tokenizer.tokens_from_list\n",
    "    \n",
    "    def featurize(self):\n",
    "        pos = [t.pos_ for s in nlp.pipe([self.items]) for t in s]   \n",
    "        \n",
    "        for token, tag in zip(self.items, pos):\n",
    "            token.pos = tag\n",
    "        \n",
    "        for token in self.items:\n",
    "            features = {\n",
    "                f\"token.lower()={token.lower()}\",\n",
    "                f\"token.is_upper()={token.is_upper}\",\n",
    "                f\"token.is_title()={token.is_title}\",\n",
    "                f\"token.is_digit()={token.is_digit}\",\n",
    "                f\"token.pos={token.pos}\",\n",
    "            }\n",
    "\n",
    "            if token.index > 0:\n",
    "                left_token = self.items[token.index - 1]\n",
    "                features.update(\n",
    "                    {\n",
    "                        f\"-1:token.lower()={left_token.lower()}\",\n",
    "                        f\"-1:token.is_title()={left_token.is_title}\",\n",
    "                        f\"-1:token.is_upper()={left_token.is_upper}\",\n",
    "                        f\"-1:token.pos={left_token.pos}\",\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                features.add(\"BOS=True\")\n",
    "\n",
    "            if token.index < max(self.indices):\n",
    "                right_token = self.items[token.index + 1]\n",
    "                features.update(\n",
    "                    {\n",
    "                        f\"+1:token.lower()={right_token.lower()}\",\n",
    "                        f\"+1:token.is_title()={right_token.is_title}\",\n",
    "                        f\"+1:token.is_upper()={right_token.is_upper}\",\n",
    "                        f\"+1:token.pos={right_token.pos}\",\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                features.add(\"EOS=True\")\n",
    "            yield features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Token 0: John>,\n",
       " <Token 1: Lennon>,\n",
       " <Token 2: was>,\n",
       " <Token 3: in>,\n",
       " <Token 4: The>,\n",
       " <Token 5: Beatles.>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'+1:token.is_title()=True',\n",
       " '+1:token.is_upper()=False',\n",
       " '+1:token.lower()=lennon',\n",
       " '+1:token.pos=NNP',\n",
       " 'BOS=True',\n",
       " 'token.is_digit()=False',\n",
       " 'token.is_title()=True',\n",
       " 'token.is_upper()=False',\n",
       " 'token.lower()=john',\n",
       " 'token.pos=NNP'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(CustomTokenSequence(tokens).featurize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sequences = [CustomTokenSequence(s.items) for s in token_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-11-30 23:16:06,431] [INFO] Loading data\n"
     ]
    }
   ],
   "source": [
    "crf = chaine.train(sequences, label_sequences, max_iterations=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
