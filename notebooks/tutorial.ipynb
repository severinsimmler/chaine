{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named entity recognition with `chaine`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chaine\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"germeval_14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence: ['Schartau', 'sagte', 'dem', '\"', 'Tagesspiegel', '\"', 'vom', 'Freitag', ',', 'Fischer', 'sei', '\"', 'in', 'einer', 'Weise', 'aufgetreten', ',', 'die', 'alles', 'andere', 'als', 'überzeugend', 'war', '\"', '.']\n",
      "Labels: [19, 0, 0, 0, 7, 0, 0, 0, 0, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sequence: {dataset['train']['tokens'][0]}\")\n",
    "print(f\"Labels: {dataset['train']['ner_tags'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform to proper sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sequences = chaine.token_sequences(dataset[\"train\"][\"tokens\"])\n",
    "label_sequences = chaine.label_sequences(dataset[\"train\"][\"ner_tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TokenSequence: [<Token 0: Schartau>, <Token 1: sagte>, <Token 2: dem>, <Token 3: \">, <Token 4: Tagesspiegel>, <Token 5: \">, <Token 6: vom>, <Token 7: Freitag>, <Token 8: ,>, <Token 9: Fischer>, <Token 10: sei>, <Token 11: \">, <Token 12: in>, <Token 13: einer>, <Token 14: Weise>, <Token 15: aufgetreten>, <Token 16: ,>, <Token 17: die>, <Token 18: alles>, <Token 19: andere>, <Token 20: als>, <Token 21: überzeugend>, <Token 22: war>, <Token 23: \">, <Token 24: .>]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(token_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<LabelSequence: ['19', '0', '0', '0', '7', '0', '0', '0', '0', '19', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(label_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model using the high-level API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-11-29 22:48:56,977] [INFO] Loading data\n",
      "[2020-11-29 22:49:05,301] [INFO] Start training\n",
      "[2020-11-29 22:49:08,678] [INFO] Iteration: 1\tLoss: 549913.612197\n",
      "[2020-11-29 22:49:09,796] [INFO] Iteration: 2\tLoss: 377078.992349\n",
      "[2020-11-29 22:49:13,021] [INFO] Iteration: 3\tLoss: 267322.682539\n",
      "[2020-11-29 22:49:15,353] [INFO] Iteration: 4\tLoss: 248339.616374\n",
      "[2020-11-29 22:49:17,698] [INFO] Iteration: 5\tLoss: 233494.412198\n",
      "[2020-11-29 22:49:18,755] [INFO] Iteration: 6\tLoss: 223343.503914\n",
      "[2020-11-29 22:49:19,795] [INFO] Iteration: 7\tLoss: 208772.524234\n",
      "[2020-11-29 22:49:20,923] [INFO] Iteration: 8\tLoss: 190119.766701\n",
      "[2020-11-29 22:49:22,212] [INFO] Iteration: 9\tLoss: 178851.893751\n",
      "[2020-11-29 22:49:23,356] [INFO] Iteration: 10\tLoss: 170839.004304\n"
     ]
    }
   ],
   "source": [
    "crf = chaine.train(token_sequences, label_sequences, max_iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sequence = next(chaine.token_sequences(dataset[\"train\"][\"tokens\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf.predict(token_sequence.featurize())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
